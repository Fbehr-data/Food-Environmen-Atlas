{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief EDA with a basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does a short Exploratory Data Analysis on the given data and introduces a basic Artificial Neural Network which performance is finally evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, mean_squared_error, multilabel_confusion_matrix\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Disregard the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we briefly look at the properties of the dataset especially its columns, rows, checking for missing values and imbalance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"data/Train_original.csv\")\n",
    "df = pd.read_csv(\"data/Train.csv\")\n",
    "df1 = pd.read_csv(\"data/Train_Dataset1.csv\")\n",
    "df2 = pd.read_csv(\"data/Train_Dataset2.csv\")\n",
    "df3 = pd.read_csv(\"data/Train_Dataset3.csv\")\n",
    "df4 = pd.read_csv(\"data/Train_Dataset4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"label\", color = 'green', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original['date'] = df_original.to_datetime(df[\"date\"])\n",
    "df_original['month'] = df_original['date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots are created to get a better understanding of the relationship between bands and the respective labels which represent the crop types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15))\n",
    "plt.subplot(231)\n",
    "sns.boxplot(x='label', y='B02', data=df_original, palette='viridis')\n",
    "\n",
    "plt.subplot(232)\n",
    "sns.boxplot(x='label', y='B03', data=df_original, palette='viridis')\n",
    "\n",
    "plt.subplot(233)\n",
    "sns.boxplot(x='label', y='B04', data=df_original, palette='viridis')\n",
    "\n",
    "plt.subplot(234)\n",
    "sns.boxplot(x='label', y='B08', data=df_original, palette='viridis')\n",
    "\n",
    "plt.subplot(235)\n",
    "sns.boxplot(x='label', y='B11', data=df_original, palette='viridis')\n",
    "\n",
    "plt.subplot(236)\n",
    "sns.boxplot(x='label', y='B12', data=df_original, palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df[df.CLM != 255.0]\n",
    "#df.date.max()\n",
    "#df.date.min()\n",
    "#df.field_id.unique()\n",
    "#df['date'] = pd.to_datetime(df[\"date\"])\n",
    "#df['month'] = df['date'].dt.month\n",
    "#vdf.CLM.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First the Features are saved in X and the target is saved as y. The feature values are used to predict the target. \n",
    "* Afterwards the dataset is split and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['B02_04', 'B02_05', 'B02_06', 'B02_07', 'B02_08', 'B02_09',\n",
    "       'B02_10', 'B02_11', 'B03_04', 'B03_05', 'B03_06', 'B03_07', 'B03_08',\n",
    "       'B03_09', 'B03_10', 'B03_11', 'B04_04', 'B04_05', 'B04_06', 'B04_07',\n",
    "       'B04_08', 'B04_09', 'B04_10', 'B04_11', 'B08_04', 'B08_05', 'B08_06',\n",
    "       'B08_07', 'B08_08', 'B08_09', 'B08_10', 'B08_11', 'B11_04', 'B11_05',\n",
    "       'B11_06', 'B11_07', 'B11_08', 'B11_09', 'B11_10', 'B11_11', 'B12_04',\n",
    "       'B12_05', 'B12_06', 'B12_07', 'B12_08', 'B12_09', 'B12_10', 'B12_11',\n",
    "       'NDVI_04', 'NDVI_05', 'NDVI_06', 'NDVI_07', 'NDVI_08', 'NDVI_09',\n",
    "       'NDVI_10', 'NDVI_11', 'WET_04', 'WET_05', 'WET_06', 'WET_07', 'WET_08',\n",
    "       'WET_09', 'WET_10', 'WET_11', 'PVR_04', 'PVR_05', 'PVR_06', 'PVR_07',\n",
    "       'PVR_08', 'PVR_09', 'PVR_10', 'PVR_11']]\n",
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loaded dataset is split into a train and test set. \n",
    "# One set is used to train the model and the other set to estimate respectively evaluate the performance of the model on new data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=150, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following a Neural Network is created with regularization and multiple layers and nodes which can be altered based on computing power. The size of the model grows with the number of nodes, the depth with the number of layers and the arrangement of the layers and nodes constitutes the architecture of the network. An optional second Network is set up with additional dropout.\n",
    "\n",
    "A node (aka neuron) is a computational unit with an input connection, a transfer function and an output connection. Nodes are then organized into layers which make up a network. A multiple-layer network is also called a Multilayer Perceptron.\n",
    "\n",
    "The input layer has to have the right number of input features - which in this notebook obviously changes from dataset to dataset. It is specified when creating the first layer-shape e.g. here as default (72,) which means 72 input variables.\n",
    "\n",
    "> Properties of the network:\n",
    "* \"Input\" layer\n",
    "* \"Dense\" layers: using the 'relu'-nonlinearity\n",
    "* \"Hidden\" layers: which means that they are not directly connected to inputs or outputs\n",
    "* \"Output\" layer: A layer of nodes that produce the output variables\n",
    "\n",
    "\n",
    "When compiling the loss function must be specified, the optimizer which searches through different weights and the metric. In the frist Model cross entropy is used as the loss argument which is suitable for classification problems (as the one here in this notebook) and is defined in Keras as “binary_crossentropy“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = len(X_train)\n",
    "# Batch: Sample(s) considered by the model within an epoch (before weights are updated).\n",
    "batch_size = 100\n",
    "# Epoch: One pass through all of the rows in the training dataset.\n",
    "epochs = 100\n",
    "STEPS_PER_EPOCH = train_n // batch_size\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.01, decay_steps=STEPS_PER_EPOCH*1000, decay_rate=1, staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.Input(shape=72,)\n",
    "a = tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=regularizers.l2(0.02))(inp)\n",
    "b= tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=regularizers.l2(0.02))(a)\n",
    "c= tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=regularizers.l2(0.02))(b)\n",
    "d= tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=regularizers.l2(0.02))(d)\n",
    "out = tf.keras.layers.Dense(500, activation = 'softmax')(d)\n",
    "\n",
    "model = tf.keras.Model(inp, out)\n",
    "model.compile(optimizer = optimizer,loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
    "history = model.fit(X_train, y_train, validation_split = 0.25, batch_size = batch_size, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(72, activation = 'relu', kernel_regularizer=regularizers.l2(0.02)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=regularizers.l2(0.02)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=regularizers.l2(0.02)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=regularizers.l2(0.02)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(500, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Mean squared error is calculated as the average of the squared differences between the predicted and actual values.\n",
    "model_two.compile(optimizer=optimizer, loss='mae', metrics=['mse'])\n",
    "history_two = model.fit(X_train, y_train, validation_split = 0.25, batch_size = batch_size, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_two.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model's performance is evaluated: accuracy, loss and F1-score. The accuracy and the loss are as well plotted.\n",
    "\n",
    "> The evaluate () function is used to generate a prediction for each input and output pair and collect scores, including the average loss and the chosen metric which is in this case accuracy. The function returns a list with two values. The first is the loss of the model on the dataset and the second is the accuracy of the model on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose = 0) \n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.subplot(211)\n",
    "plt.title('Loss Diagram')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(epochs), history.history['loss'], label = 'Training loss')\n",
    "plt.plot(range(epochs), history.history['val_loss'], label = 'Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy Diagram')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Test')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions and getting the F1 score.\n",
    "\n",
    "y_predict = np.argmax(model.predict(X_test), axis=-1)\n",
    "f1_score(y_test, y_predict,average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this further analysis  of the second model the values are first converted into Numpy arrays and then based on the second model the MSE is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_two = {}\n",
    "history_two = model_two.fit(X_train, y_train, validation_split=0.25, verbose=0, steps_per_epoch=STEPS_PER_EPOCH, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_two.history['mse'])\n",
    "plt.plot(history_two.history['val_mse'])\n",
    "plt.title('Model MSE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0408a114c6509f724dbd951c4fa7c6f2fa15147806242461b8de14c803dba44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
