{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,random,pickle,time,glob,sys, multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict\n",
    "from engineering_functions import mean_per_band_per_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory and the chunks in which the larger fields are splitted \n",
    "DATA_DIR = '../data'\n",
    "MAX_CHUNKS = int(1)\n",
    "DIR_BANDS = f'{DATA_DIR}/train/bands-raw/' \n",
    "# Load the data frame and add the path information of the npz objects for each field to the data frame\n",
    "df = pd.read_pickle(f'{DATA_DIR}/train/field_meta_train.pkl')\n",
    "df['path'] = DIR_BANDS+df.field_id.astype(str)+'.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>tile_id</th>\n",
       "      <th>label</th>\n",
       "      <th>dates</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2171</td>\n",
       "      <td>4</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-11T00:...</td>\n",
       "      <td>../data/train/bands-raw/1.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1703</td>\n",
       "      <td>7</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-11T00:...</td>\n",
       "      <td>../data/train/bands-raw/2.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2214</td>\n",
       "      <td>6</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-11T00:...</td>\n",
       "      <td>../data/train/bands-raw/3.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2526</td>\n",
       "      <td>8</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-11T00:...</td>\n",
       "      <td>../data/train/bands-raw/4.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>544</td>\n",
       "      <td>4</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-11T00:...</td>\n",
       "      <td>../data/train/bands-raw/6.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87087</th>\n",
       "      <td>122731</td>\n",
       "      <td>2298</td>\n",
       "      <td>4</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-11T00:...</td>\n",
       "      <td>../data/train/bands-raw/122731.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87088</th>\n",
       "      <td>122732</td>\n",
       "      <td>2225</td>\n",
       "      <td>5</td>\n",
       "      <td>[2017-04-04T00:00:00.000000000, 2017-04-14T00:...</td>\n",
       "      <td>../data/train/bands-raw/122732.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87089</th>\n",
       "      <td>122733</td>\n",
       "      <td>1986</td>\n",
       "      <td>2</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-11T00:...</td>\n",
       "      <td>../data/train/bands-raw/122733.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87090</th>\n",
       "      <td>122735</td>\n",
       "      <td>997</td>\n",
       "      <td>3</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-04T00:...</td>\n",
       "      <td>../data/train/bands-raw/122735.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87091</th>\n",
       "      <td>122736</td>\n",
       "      <td>375</td>\n",
       "      <td>9</td>\n",
       "      <td>[2017-04-01T00:00:00.000000000, 2017-04-04T00:...</td>\n",
       "      <td>../data/train/bands-raw/122736.npz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87092 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       field_id  tile_id  label  \\\n",
       "0             1     2171      4   \n",
       "1             2     1703      7   \n",
       "2             3     2214      6   \n",
       "3             4     2526      8   \n",
       "4             6      544      4   \n",
       "...         ...      ...    ...   \n",
       "87087    122731     2298      4   \n",
       "87088    122732     2225      5   \n",
       "87089    122733     1986      2   \n",
       "87090    122735      997      3   \n",
       "87091    122736      375      9   \n",
       "\n",
       "                                                   dates  \\\n",
       "0      [2017-04-01T00:00:00.000000000, 2017-04-11T00:...   \n",
       "1      [2017-04-01T00:00:00.000000000, 2017-04-11T00:...   \n",
       "2      [2017-04-01T00:00:00.000000000, 2017-04-11T00:...   \n",
       "3      [2017-04-01T00:00:00.000000000, 2017-04-11T00:...   \n",
       "4      [2017-04-01T00:00:00.000000000, 2017-04-11T00:...   \n",
       "...                                                  ...   \n",
       "87087  [2017-04-01T00:00:00.000000000, 2017-04-11T00:...   \n",
       "87088  [2017-04-04T00:00:00.000000000, 2017-04-14T00:...   \n",
       "87089  [2017-04-01T00:00:00.000000000, 2017-04-11T00:...   \n",
       "87090  [2017-04-01T00:00:00.000000000, 2017-04-04T00:...   \n",
       "87091  [2017-04-01T00:00:00.000000000, 2017-04-04T00:...   \n",
       "\n",
       "                                     path  \n",
       "0           ../data/train/bands-raw/1.npz  \n",
       "1           ../data/train/bands-raw/2.npz  \n",
       "2           ../data/train/bands-raw/3.npz  \n",
       "3           ../data/train/bands-raw/4.npz  \n",
       "4           ../data/train/bands-raw/6.npz  \n",
       "...                                   ...  \n",
       "87087  ../data/train/bands-raw/122731.npz  \n",
       "87088  ../data/train/bands-raw/122732.npz  \n",
       "87089  ../data/train/bands-raw/122733.npz  \n",
       "87090  ../data/train/bands-raw/122735.npz  \n",
       "87091  ../data/train/bands-raw/122736.npz  \n",
       "\n",
       "[87092 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting data from 87092\n",
      "processesing on : 8 cpus\n",
      "Task # 1 process tiles 10886\n",
      "Task # 2 process tiles 10887\n",
      "Task # 3 process tiles 10886\n",
      "Task # 4 process tiles 10887\n",
      "Task # 5 process tiles 10886\n",
      "Task # 6 process tiles 10887\n",
      "Task # 7 process tiles 10886\n",
      "Task # 8 process tiles 10887\n"
     ]
    }
   ],
   "source": [
    "# Create a sorted dataframe by the field ids\n",
    "field_ids = sorted(df.field_id.unique())\n",
    "print(f'extracting data from {len(df.field_id.unique())}')\n",
    "\n",
    "# Check the number of CPU cores\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(f'processesing on : {num_processes} cpus')\n",
    "\n",
    "# Create a pool of processes equal to the number of cores\n",
    "pool = multiprocessing.Pool(num_processes)\n",
    "# Calculate the number of fields each core must process\n",
    "field_per_process = len(df.field_id.unique()) / num_processes\n",
    "# Create the a number of field id batches equal to the number of cores\n",
    "batches = []\n",
    "for num_process in range(1, num_processes + 1):\n",
    "    start_index = (num_process - 1) * field_per_process + 1\n",
    "    end_index = num_process * field_per_process\n",
    "    start_index = int(start_index)\n",
    "    end_index = int(end_index)\n",
    "    sublist = df.field_id.unique()[start_index - 1:end_index]\n",
    "    batches.append((sublist,))\n",
    "    print(f\"Task # {num_process} process tiles {len(sublist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87092"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10886/10886 [00:36<00:00, 297.82it/s]\n",
      "100%|██████████| 10887/10887 [00:35<00:00, 310.48it/s]\n",
      " 12%|█▏        | 1265/10886 [00:04<00:31, 304.39it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Error -3 while decompressing data: invalid block type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb#ch0000002?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batches:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb#ch0000002?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _,row \u001b[39min\u001b[39;00m tqdm(df\u001b[39m.\u001b[39miloc[batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()]\u001b[39m.\u001b[39miterrows(),total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df\u001b[39m.\u001b[39miloc[batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()])):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb#ch0000002?line=8'>9</a>\u001b[0m         bands \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(row\u001b[39m.\u001b[39;49mpath)[\u001b[39m'\u001b[39;49m\u001b[39marr_0\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb#ch0000002?line=10'>11</a>\u001b[0m         n \u001b[39m=\u001b[39m bands\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb#ch0000002?line=11'>12</a>\u001b[0m         n_dates \u001b[39m=\u001b[39m bands\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/neuefische/Radiant-Earth-Spot-Crop/.venv/lib/python3.9/site-packages/numpy/lib/npyio.py:241\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m member:\n\u001b[1;32m    240\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 241\u001b[0m     magic \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(\u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mMAGIC_PREFIX))\n\u001b[1;32m    242\u001b[0m     \u001b[39mbytes\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/zipfile.py:922\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    921\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[0;32m--> 922\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[1;32m    923\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m    924\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/zipfile.py:998\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m    997\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m--> 998\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[1;32m    999\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1002\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[0;31merror\u001b[0m: Error -3 while decompressing data: invalid block type"
     ]
    }
   ],
   "source": [
    "field_ids = []\n",
    "labels = []\n",
    "sub_ids = []\n",
    "dates = []\n",
    "features = []\n",
    "\n",
    "for batch in batches:\n",
    "    for _,row in tqdm(df.iloc[batch[0].tolist()].iterrows(),total=len(df.iloc[batch[0].tolist()])):\n",
    "        bands = np.load(row.path)['arr_0']\n",
    "\n",
    "        n = bands.shape[0]\n",
    "        n_dates = bands.shape[2]\n",
    "        num_chunks = MAX_CHUNKS\n",
    "        if n<MAX_CHUNKS*2 or MAX_CHUNKS==1:\n",
    "            num_chunks = 1\n",
    "            mean = np.mean(bands,axis=0)\n",
    "        else:\n",
    "            bands = np.array_split(bands,num_chunks)\n",
    "            mean = [np.mean(x,axis=0) for x in bands]\n",
    "            mean = np.concatenate(mean,axis=1)\n",
    "\n",
    "        feature = np.concatenate([mean]).transpose(1,0)\n",
    "        features.append(feature)\n",
    "\n",
    "        field_id = np.repeat(row.field_id,len(features))\n",
    "        field_ids.append(field_id)\n",
    "        label = np.repeat(row.label,len(features))\n",
    "        labels.append(label)\n",
    "        \n",
    "        ids = [i for i in range(num_chunks)]\n",
    "        dts = [str(d)[:10] for d in row.dates]\n",
    "        dts = np.tile(dts,num_chunks)\n",
    "        dates.append(dts)\n",
    "        ids = np.repeat([ids],len(row.dates))\n",
    "        sub_ids.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Error -3 while decompressing data: invalid block type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maxlanger/neuefische/Radiant-Earth-Spot-Crop/data_engineering/preprocessing_data.ipynb#ch0000021?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49mload(row\u001b[39m.\u001b[39;49mpath)[\u001b[39m'\u001b[39;49m\u001b[39marr_0\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m~/neuefische/Radiant-Earth-Spot-Crop/.venv/lib/python3.9/site-packages/numpy/lib/npyio.py:241\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m member:\n\u001b[1;32m    240\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 241\u001b[0m     magic \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(\u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mMAGIC_PREFIX))\n\u001b[1;32m    242\u001b[0m     \u001b[39mbytes\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/zipfile.py:922\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    921\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[0;32m--> 922\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[1;32m    923\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m    924\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/zipfile.py:998\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m    997\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m--> 998\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[1;32m    999\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1002\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[0;31merror\u001b[0m: Error -3 while decompressing data: invalid block type"
     ]
    }
   ],
   "source": [
    "np.load(row.path)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.concatenate(features)\n",
    "all_field_ids = np.concatenate(field_ids)\n",
    "all_labels = np.concatenate(labels)\n",
    "\n",
    "all_sub_ids = np.concatenate(sub_ids)\n",
    "all_dates = np.concatenate(dates)\n",
    "\n",
    "cols = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08','B8A', 'B09', 'B11', 'B12', 'CLM']\n",
    "df_data = pd.DataFrame(all_features,columns=cols)\n",
    "df_data.insert(0,'field_id',all_field_ids)\n",
    "df_data.insert(1,'sub_id',all_sub_ids)\n",
    "df_data.insert(2,'date',all_dates)\n",
    "df_data.insert(3,'label',all_labels)\n",
    "\n",
    "df_data['field_id'] = df_data['field_id'].astype(np.int32)\n",
    "df_data['sub_id'] = df_data['sub_id'].astype(np.int8)\n",
    "fn = f'{DATA_DIR}/s2_train_dxc{MAX_CHUNKS}_bands.h5'\n",
    "df_data.to_hdf(fn,key='df')\n",
    "print(f'saved data to {fn}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f09b4927e4ba45dbad33b3bb266e4177c67ff52e8879550fc4abd145d1e96f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
